{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bc902c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=(128, 128), frames_per_clip=16, transform=None):\n",
    "        self.img_size = img_size\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        # Scan dataset\n",
    "        all_labels = []\n",
    "        for label in os.listdir(data_dir):\n",
    "            label_path = os.path.join(data_dir, label)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "            frames = sorted(os.listdir(label_path))\n",
    "            if len(frames) < frames_per_clip:\n",
    "                continue  # skip too short videos\n",
    "            # save sequence of frames as one sample\n",
    "            self.samples.append([os.path.join(label_path, f) for f in frames])\n",
    "            all_labels.append(label)\n",
    "\n",
    "        self.labels = self.label_encoder.fit_transform(all_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # pick evenly spaced frames\n",
    "        step = max(1, len(frame_paths) // self.frames_per_clip)\n",
    "        selected = frame_paths[::step][:self.frames_per_clip]\n",
    "\n",
    "        clip = []\n",
    "        for f in selected:\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img = img / 255.0\n",
    "            clip.append(img)\n",
    "\n",
    "        clip = np.array(clip)  # (T, H, W, C)\n",
    "        clip = np.transpose(clip, (3, 0, 1, 2))  # (C, T, H, W) for 3D CNN\n",
    "        return torch.tensor(clip, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc251b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class ActionRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # 3D CNN backbone\n",
    "        self.cnn3d = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1,2,2)),\n",
    "\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((2,2,2)),\n",
    "\n",
    "            nn.Conv3d(128, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1))  # keep T, collapse H,W\n",
    "        )\n",
    "\n",
    "        # Transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, T, H, W)\n",
    "        feats = self.cnn3d(x)           # (B, E, T, 1, 1)\n",
    "        feats = feats.squeeze(-1).squeeze(-1)  # (B, E, T)\n",
    "\n",
    "        feats = feats.permute(2, 0, 1)  # (T, B, E) for transformer\n",
    "        out = self.transformer(feats)   # (T, B, E)\n",
    "\n",
    "        out = out.mean(0)               # (B, E) temporal average\n",
    "        return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63024149",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train_model(data_dir, epochs=10, batch_size=4, lr=1e-4):\n",
    "    dataset = VideoDataset(data_dir)\n",
    "    num_classes = len(np.unique(dataset.labels))\n",
    "\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ActionRecognitionModel(num_classes=num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for clips, labels in loader:\n",
    "            clips, labels = clips.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(clips)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, pred = outputs.max(1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}, Acc: {correct/total:.4f}\")\n",
    "\n",
    "    return model, dataset.label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589f6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, encoder, video_dir, frames_per_clip=16, img_size=(128,128)):\n",
    "    model.eval()\n",
    "    frames = sorted(os.listdir(video_dir))\n",
    "    step = max(1, len(frames) // frames_per_clip)\n",
    "    selected = frames[::step][:frames_per_clip]\n",
    "\n",
    "    clip = []\n",
    "    for f in selected:\n",
    "        img = cv2.imread(os.path.join(video_dir, f))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0\n",
    "        clip.append(img)\n",
    "\n",
    "    clip = np.array(clip)\n",
    "    clip = np.transpose(clip, (3,0,1,2))\n",
    "    clip = torch.tensor(clip, dtype=torch.float32).unsqueeze(0)  # (1,C,T,H,W)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(clip.to(next(model.parameters()).device))\n",
    "        pred = outputs.argmax(1).item()\n",
    "    return encoder.inverse_transform([pred])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11d673e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pramananda/working_dir/swe/projects/daily-life-tracker/env/lib/python3.13/site-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 2.9996, Acc: 0.0000\n",
      "Epoch 2/5, Loss: 1.4855, Acc: 0.3333\n",
      "Epoch 3/5, Loss: 1.5639, Acc: 0.3333\n",
      "Epoch 4/5, Loss: 1.2378, Acc: 0.0000\n",
      "Epoch 5/5, Loss: 1.1873, Acc: 0.3333\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/dataset/walking'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 162\u001b[39m\n\u001b[32m    158\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted activity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Inference example (use any video folder with frames)\u001b[39;00m\n\u001b[32m    156\u001b[39m test_video_dir = os.path.join(data_dir, \u001b[33m\"\u001b[39m\u001b[33mwalking\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# change to your test video path\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m prediction = \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_video_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    158\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPredicted activity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprediction\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 125\u001b[39m, in \u001b[36mpredict\u001b[39m\u001b[34m(model, encoder, video_dir, frames_per_clip, img_size)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(model, encoder, video_dir, frames_per_clip=\u001b[32m16\u001b[39m, img_size=(\u001b[32m128\u001b[39m, \u001b[32m128\u001b[39m)):\n\u001b[32m    124\u001b[39m     model.eval()\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     frames = \u001b[38;5;28msorted\u001b[39m(\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    126\u001b[39m     step = \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(frames) // frames_per_clip)\n\u001b[32m    127\u001b[39m     selected = frames[::step][:frames_per_clip]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/dataset/walking'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# -------------------- Dataset --------------------\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, data_dir, img_size=(128, 128), frames_per_clip=16):\n",
    "        self.img_size = img_size\n",
    "        self.frames_per_clip = frames_per_clip\n",
    "        self.samples = []\n",
    "        self.labels = []\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        all_labels = []\n",
    "        for label in os.listdir(data_dir):\n",
    "            label_path = os.path.join(data_dir, label)\n",
    "            if not os.path.isdir(label_path):\n",
    "                continue\n",
    "            frames = sorted(os.listdir(label_path))\n",
    "            if len(frames) < frames_per_clip:\n",
    "                continue\n",
    "            self.samples.append([os.path.join(label_path, f) for f in frames])\n",
    "            all_labels.append(label)\n",
    "\n",
    "        self.labels = self.label_encoder.fit_transform(all_labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        frame_paths = self.samples[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        step = max(1, len(frame_paths) // self.frames_per_clip)\n",
    "        selected = frame_paths[::step][:self.frames_per_clip]\n",
    "\n",
    "        clip = []\n",
    "        for f in selected:\n",
    "            img = cv2.imread(f)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, self.img_size)\n",
    "            img = img / 255.0\n",
    "            clip.append(img)\n",
    "\n",
    "        clip = np.array(clip)  # (T, H, W, C)\n",
    "        clip = np.transpose(clip, (3, 0, 1, 2))  # (C, T, H, W)\n",
    "        return torch.tensor(clip, dtype=torch.float32), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "\n",
    "# -------------------- Model --------------------\n",
    "class ActionRecognitionModel(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim=256, num_heads=4, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cnn3d = nn.Sequential(\n",
    "            nn.Conv3d(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((1, 2, 2)),\n",
    "\n",
    "            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d((2, 2, 2)),\n",
    "\n",
    "            nn.Conv3d(128, embed_dim, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool3d((None, 1, 1))\n",
    "        )\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.cnn3d(x)           # (B, E, T, 1, 1)\n",
    "        feats = feats.squeeze(-1).squeeze(-1)  # (B, E, T)\n",
    "        feats = feats.permute(2, 0, 1)  # (T, B, E)\n",
    "        out = self.transformer(feats)   # (T, B, E)\n",
    "        out = out.mean(0)               # (B, E)\n",
    "        return self.fc(out)\n",
    "\n",
    "\n",
    "# -------------------- Train --------------------\n",
    "def train_model(data_dir, epochs=5, batch_size=2, lr=1e-4):\n",
    "    dataset = VideoDataset(data_dir)\n",
    "    num_classes = len(np.unique(dataset.labels))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ActionRecognitionModel(num_classes=num_classes).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss, correct, total = 0, 0, 0\n",
    "        for clips, labels in loader:\n",
    "            clips, labels = clips.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(clips)\n",
    "            loss = F.cross_entropy(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, pred = outputs.max(1)\n",
    "            correct += pred.eq(labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}, Acc: {correct/total:.4f}\")\n",
    "\n",
    "    return model, dataset.label_encoder\n",
    "\n",
    "\n",
    "# -------------------- Inference --------------------\n",
    "def predict(model, encoder, video_dir, frames_per_clip=16, img_size=(128, 128)):\n",
    "    model.eval()\n",
    "    frames = sorted(os.listdir(video_dir))\n",
    "    step = max(1, len(frames) // frames_per_clip)\n",
    "    selected = frames[::step][:frames_per_clip]\n",
    "\n",
    "    clip = []\n",
    "    for f in selected:\n",
    "        img = cv2.imread(os.path.join(video_dir, f))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img = img / 255.0\n",
    "        clip.append(img)\n",
    "\n",
    "    clip = np.array(clip)\n",
    "    clip = np.transpose(clip, (3, 0, 1, 2))\n",
    "    clip = torch.tensor(clip, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(clip.to(next(model.parameters()).device))\n",
    "        pred = outputs.argmax(1).item()\n",
    "    return encoder.inverse_transform([pred])[0]\n",
    "\n",
    "\n",
    "# -------------------- Main --------------------\n",
    "def main():\n",
    "    data_dir = \"data/dataset\"  # put your dataset path here\n",
    "    model, encoder = train_model(data_dir, epochs=5, batch_size=2)\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), \"activity_model.pth\")\n",
    "\n",
    "    # Inference example (use any video folder with frames)\n",
    "    test_video_dir = os.path.join(data_dir, \"walking\")  # change to your test video path\n",
    "    prediction = predict(model, encoder, test_video_dir)\n",
    "    print(f\"Predicted activity: {prediction}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cca8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da989dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
